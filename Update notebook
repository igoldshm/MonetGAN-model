{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T21:31:50.799899Z","iopub.execute_input":"2025-04-06T21:31:50.800279Z","iopub.status.idle":"2025-04-06T21:32:06.106276Z","shell.execute_reply.started":"2025-04-06T21:31:50.800250Z","shell.execute_reply":"2025-04-06T21:32:06.105442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"monet_path = '/kaggle/input/gan-getting-started/monet_jpg/'\nphoto_path = '/kaggle/input/gan-getting-started/photo_jpg/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T21:32:06.107316Z","iopub.execute_input":"2025-04-06T21:32:06.107709Z","iopub.status.idle":"2025-04-06T21:32:06.111191Z","shell.execute_reply.started":"2025-04-06T21:32:06.107686Z","shell.execute_reply":"2025-04-06T21:32:06.110346Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((256,256)),transforms.ToTensor(),\n    transforms.Normalize((0.5),(0.5))\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T21:32:06.112565Z","iopub.execute_input":"2025-04-06T21:32:06.112866Z","iopub.status.idle":"2025-04-06T21:32:06.126573Z","shell.execute_reply.started":"2025-04-06T21:32:06.112834Z","shell.execute_reply":"2025-04-06T21:32:06.125939Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MonetDataset(Dataset):\n    def __init__(self, monet_dir, photo_dir, transform=None):\n        self.monet_files = [os.path.join(monet_dir, f) for f in os.listdir(monet_dir)]\n        self.photo_files = [os.path.join(photo_dir, f) for f in os.listdir(photo_dir)]\n        self.transform = transform\n\n    def __len__(self):\n        return min(len(self.monet_files), len(self.photo_files))  # Match dataset size\n\n    def __getitem__(self, idx):\n        monet_img = Image.open(self.monet_files[idx]).convert(\"RGB\")\n        photo_img = Image.open(self.photo_files[idx]).convert(\"RGB\")\n\n        if self.transform:\n            monet_img = self.transform(monet_img)\n            photo_img = self.transform(photo_img)\n\n        return monet_img, photo_img\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T21:32:06.127616Z","iopub.execute_input":"2025-04-06T21:32:06.127797Z","iopub.status.idle":"2025-04-06T21:32:06.140347Z","shell.execute_reply.started":"2025-04-06T21:32:06.127781Z","shell.execute_reply":"2025-04-06T21:32:06.139584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Load the data\ndataset = MonetDataset(monet_path, photo_path, transform=transform)\ndataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T21:32:06.141273Z","iopub.execute_input":"2025-04-06T21:32:06.141637Z","iopub.status.idle":"2025-04-06T21:32:06.166175Z","shell.execute_reply.started":"2025-04-06T21:32:06.141609Z","shell.execute_reply":"2025-04-06T21:32:06.165374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Let's visualize the data\nimport matplotlib.pyplot as plt\nsample_monet, sample_photo = next(iter(dataloader))\nfig, ax = plt.subplots(1, 2)\nax[0].imshow(sample_monet.squeeze().permute(1, 2, 0) * 0.5 + 0.5)\nax[0].set_title(\"Monet Painting\")\nax[1].imshow(sample_photo.squeeze().permute(1, 2, 0) * 0.5 + 0.5)\nax[1].set_title(\"Real Photo\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T21:32:06.166972Z","iopub.execute_input":"2025-04-06T21:32:06.167243Z","iopub.status.idle":"2025-04-06T21:32:06.962439Z","shell.execute_reply.started":"2025-04-06T21:32:06.167217Z","shell.execute_reply":"2025-04-06T21:32:06.961520Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Generator class implementation (U-Net based)","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass ResBlock(nn.Module):\n    def __init__(self, channels):\n        super(ResBlock, self).__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n            nn.InstanceNorm2d(channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n            nn.InstanceNorm2d(channels),\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        \n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=3),\n            nn.InstanceNorm2d(64),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n            nn.InstanceNorm2d(128),\n            nn.ReLU(inplace=True),\n            \n            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n            nn.InstanceNorm2d(256),\n            nn.ReLU(inplace=True),\n        )\n\n        # ResNet blocks\n        self.resnet_blocks = nn.Sequential(*[ResBlock(256) for _ in range(9)])\n\n        # Decoder\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.InstanceNorm2d(128),\n            nn.ReLU(inplace=True),\n\n            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.InstanceNorm2d(64),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(64, 3, kernel_size=7, stride=1, padding=3),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.resnet_blocks(x)\n        x = self.decoder(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T21:32:06.963300Z","iopub.execute_input":"2025-04-06T21:32:06.963532Z","iopub.status.idle":"2025-04-06T21:32:06.972241Z","shell.execute_reply.started":"2025-04-06T21:32:06.963513Z","shell.execute_reply":"2025-04-06T21:32:06.971505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def init_weights(net, init_gain=0.02):\n    for m in net.modules():\n        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n            nn.init.normal_(m.weight.data, 0.0, init_gain)\n            if m.bias is not None:\n                nn.init.constant_(m.bias.data, 0.0)\n        elif isinstance(m, nn.InstanceNorm2d):\n            if m.weight is not None:\n                nn.init.normal_(m.weight.data, 1.0, init_gain)\n            if m.bias is not None:\n                nn.init.constant_(m.bias.data, 0.0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T21:32:06.974281Z","iopub.execute_input":"2025-04-06T21:32:06.974516Z","iopub.status.idle":"2025-04-06T21:32:06.995864Z","shell.execute_reply.started":"2025-04-06T21:32:06.974496Z","shell.execute_reply":"2025-04-06T21:32:06.995108Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Discriminator class implemintation","metadata":{}},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, in_channels=3):\n        super(Discriminator, self).__init__()\n\n        def conv_block(in_c, out_c, normalize=True, apply_dropout=False):\n            layers = [nn.Conv2d(in_c, out_c, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_c))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            if apply_dropout:\n                layers.append(nn.Dropout2d(0.3))  # or 0.5\n            return layers\n\n        self.model = nn.Sequential(\n            *conv_block(in_channels, 64, normalize=False),      # no norm/dropout\n            *conv_block(64, 128, apply_dropout=True),           # add dropout here\n            *conv_block(128, 256, apply_dropout=True),          # and here\n            nn.Conv2d(256, 1, 4, padding=1)  # Output 1-channel \"real/fake\" map\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n\ndiscriminator_A = Discriminator()  # Monet Discriminator\ndiscriminator_B = Discriminator()  # Photo Discriminator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T21:32:06.997079Z","iopub.execute_input":"2025-04-06T21:32:06.997572Z","iopub.status.idle":"2025-04-06T21:32:07.025546Z","shell.execute_reply.started":"2025-04-06T21:32:06.997540Z","shell.execute_reply":"2025-04-06T21:32:07.024945Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_generated_samples(generator, real_photo, epoch):\n    generator.eval() #Set the generator to evaluation mode\n    with torch.no_grad():\n        fake_monet = generator(real_photo.to(device)).cpu()\n    #Convert tensors to displayable images\n    real_photo = real_photo.squeeze().permute(1, 2, 0)*0.5 + 0.5\n    fake_monet = fake_monet.squeeze().permute(1, 2, 0)*0.5 + 0.5\n\n    #plot real & fake images\n    fig, ax = plt.subplots(1, 2, figsize=(8,4))\n    ax[0].imshow(real_photo.numpy())\n    ax[0].set_title(\"Real Photo\")\n    ax[0].axis(\"off\")\n\n    ax[1].imshow(fake_monet.numpy())\n    ax[1].set_title(\"Generated Monet Painting\")\n    ax[1].axis(\"off\")\n\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T21:32:07.026273Z","iopub.execute_input":"2025-04-06T21:32:07.026566Z","iopub.status.idle":"2025-04-06T21:32:07.031546Z","shell.execute_reply.started":"2025-04-06T21:32:07.026538Z","shell.execute_reply":"2025-04-06T21:32:07.030832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ReplayBuffer():\n    def __init__(self, max_size=50):\n        self.max_size = max_size\n        self.data = []\n\n    def push_and_pop(self, data):\n        result = []\n        for element in data.data:\n            element = torch.unsqueeze(element, 0)\n            if len(self.data) < self.max_size:\n                self.data.append(element)\n                result.append(element)\n            else:\n                if torch.rand(1).item() > 0.5:\n                    idx = torch.randint(0, len(self.data), (1,)).item()\n                    result.append(self.data[idx].clone())\n                    self.data[idx] = element\n                else:\n                    result.append(element)\n        return torch.cat(result)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T21:32:07.032253Z","iopub.execute_input":"2025-04-06T21:32:07.032546Z","iopub.status.idle":"2025-04-06T21:32:07.048268Z","shell.execute_reply.started":"2025-04-06T21:32:07.032518Z","shell.execute_reply":"2025-04-06T21:32:07.047496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fake_A_buffer = ReplayBuffer()\nfake_B_buffer = ReplayBuffer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T21:32:07.048955Z","iopub.execute_input":"2025-04-06T21:32:07.049227Z","iopub.status.idle":"2025-04-06T21:32:07.060988Z","shell.execute_reply.started":"2025-04-06T21:32:07.049203Z","shell.execute_reply":"2025-04-06T21:32:07.060345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom tqdm import tqdm\n\n# Device setup (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize Generator (Photo → Monet) and Discriminator (Monet)\nG_A2B = Generator().to(device)  # Photo → Monet\ninit_weights(G_A2B)\nG_B2A = Generator().to(device)  # Monet → Photo\ninit_weights(G_B2A)\nD_A = Discriminator().to(device)  # Monet Discriminator\nD_B = Discriminator().to(device)  # Photo Discriminator\n\n# Loss functions\ncriterion_GAN = nn.MSELoss()  # Adversarial loss\ncriterion_cycle = nn.L1Loss()  # Cycle consistency loss\ncriterion_identity = nn.L1Loss()  # Identity loss\n\n# Optimizers\noptimizer_G = optim.Adam(\n    list(G_A2B.parameters()) + list(G_B2A.parameters()), lr=0.0002, betas=(0.5, 0.999)\n)\noptimizer_D_A = optim.Adam(D_A.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizer_D_B = optim.Adam(D_B.parameters(), lr=0.0002, betas=(0.5, 0.999))\n\n# Training parameters\nEPOCHS = 100\nLAMBDA_CYCLE = 10  # Weight for cycle consistency loss (adjusted)\nLAMBDA_IDENTITY = 0.5  # Weight for identity loss (adjusted)\n\n# Learning rate schedulers (linear decay after half of total epochs)\nlr_lambda = lambda epoch: 1.0 - max(0, epoch - EPOCHS // 2) / float(EPOCHS // 2)\nlr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=lr_lambda)\nlr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=lr_lambda)\nlr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=lr_lambda)\n\n# Training Loop\nfor epoch in range(EPOCHS):\n    loop = tqdm(dataloader, leave=True)\n    \n    for i, (real_A, _) in enumerate(loop):  # Only use photos, ignore Monet images\n        real_A = real_A.to(device)  # Real photo\n        real_B = real_A.clone().to(device)  # Fake Monet (used for identity loss)\n\n        ### ---- TRAIN GENERATOR (Photo → Monet) ---- ###\n        optimizer_G.zero_grad()\n\n        # Generate fake Monet from real photo\n        fake_B = G_A2B(real_A)\n        pred_fake_B = D_A(fake_B)  # Discriminator result on fake Monet\n\n        # Adversarial Loss for Generator G_A2B (Photo → Monet)\n        loss_GAN_A2B = criterion_GAN(pred_fake_B, torch.ones_like(pred_fake_B))\n\n        # Identity Loss (Monet → Monet should remain unchanged)\n        identity_B = G_A2B(real_B)\n        loss_identity_B = criterion_identity(real_B, identity_B) * LAMBDA_IDENTITY\n\n        # Cycle Consistency Loss (Photo → Monet → Photo)\n        rec_A = G_B2A(fake_B)  # Reconstruct photo from fake Monet (using G_B2A)\n        loss_cycle_A = criterion_cycle(real_A, rec_A) * LAMBDA_CYCLE  # Apply cycle consistency weight\n\n        # Total Generator Loss for G_A2B\n        loss_G_A2B = loss_GAN_A2B + loss_cycle_A + loss_identity_B\n        loss_G_A2B.backward()\n\n        ### ---- TRAIN GENERATOR (Monet → Photo) ---- ###\n        optimizer_G.zero_grad()\n\n        # Generate fake photo from real Monet\n        fake_A = G_B2A(real_B)  \n        pred_fake_A = D_B(fake_A)  # Discriminator result on fake Photo\n\n        # Adversarial Loss for Generator G_B2A (Monet → Photo)\n        loss_GAN_B2A = criterion_GAN(pred_fake_A, torch.ones_like(pred_fake_A))\n\n        # Identity Loss (Photo → Photo should remain unchanged)\n        identity_A = G_B2A(real_A)\n        loss_identity_A = criterion_identity(real_A, identity_A) * LAMBDA_IDENTITY\n\n        # Cycle Consistency Loss (Monet → Photo → Monet)\n        rec_B = G_A2B(fake_A)  # Reconstruct Monet from fake photo (using G_A2B)\n        loss_cycle_B = criterion_cycle(real_B, rec_B) * LAMBDA_CYCLE  # Apply cycle consistency weight\n\n        # Total Generator Loss for G_B2A\n        loss_G_B2A = loss_GAN_B2A + loss_cycle_B + loss_identity_A\n        loss_G_B2A.backward()\n\n        # Now update both generators\n        optimizer_G.step()\n\n        ### ---- TRAIN DISCRIMINATOR (Monet) ---- ###\n        optimizer_D_A.zero_grad()\n\n        # Discriminator Loss: Real Monet\n        pred_real_B = D_A(real_B)\n        loss_D_real_B = criterion_GAN(pred_real_B, torch.full_like(pred_real_B, 0.9))\n\n        # Discriminator Loss: Fake Monet\n        #pred_fake_B = D_A(fake_B.detach())  # Stop gradient on generator\n        fake_B_buffered = fake_B_buffer.push_and_pop(fake_B)\n        pred_fake_B = D_A(fake_B_buffered)\n        loss_D_fake_B = criterion_GAN(pred_fake_B, torch.zeros_like(pred_fake_B))\n\n        # Total Discriminator Loss for D_A (Monet Discriminator)\n        loss_D_A = (loss_D_real_B + loss_D_fake_B) * 0.5\n        loss_D_A.backward()\n        optimizer_D_A.step()\n\n        ### ---- TRAIN DISCRIMINATOR (Photo) ---- ###\n        optimizer_D_B.zero_grad()\n\n        # Discriminator Loss: Real Photo\n        pred_real_A = D_B(real_A)\n        loss_D_real_A = criterion_GAN(pred_real_A, torch.full_like(pred_real_A, 0.9))\n\n        # Discriminator Loss: Fake Photo\n        #pred_fake_A = D_B(fake_A.detach())  # Stop gradient on generator\n        fake_A_buffered = fake_A_buffer.push_and_pop(fake_A)\n        pred_fake_A = D_A(fake_A_buffered)\n        loss_D_fake_A = criterion_GAN(pred_fake_A, torch.zeros_like(pred_fake_A))\n\n        # Total Discriminator Loss for D_B (Photo Discriminator)\n        loss_D_B = (loss_D_real_A + loss_D_fake_A) * 0.5\n        loss_D_B.backward()\n        optimizer_D_B.step()\n\n        ### ---- Update Progress Bar ---- ###\n        loop.set_description(f\"Epoch [{epoch+1}/{EPOCHS}]\")\n        loop.set_postfix(\n            G_loss_A2B=loss_G_A2B.item(), G_loss_B2A=loss_G_B2A.item(), D_loss_A=loss_D_A.item(), D_loss_B=loss_D_B.item()\n        )\n    lr_scheduler_G.step()\n    lr_scheduler_D_A.step()\n    lr_scheduler_D_B.step()\n\n    # Save and show generated samples periodically (every 5 epochs, for example)\n    if (epoch + 1) % 5 == 0:\n        _, real_photo_sample = next(iter(dataloader))\n        show_generated_samples(G_A2B, real_photo_sample[:1], epoch + 1)\n    \n    # Save checkpoints every 10 epochs\n    if (epoch + 1) % 10 == 0:\n        torch.save(G_A2B.state_dict(), f\"generator_photo2monet_epoch{epoch+1}.pth\")\n        torch.save(G_B2A.state_dict(), f\"generator_monet2photo_epoch{epoch+1}.pth\")\n        torch.save(D_A.state_dict(), f\"discriminator_monet_epoch{epoch+1}.pth\")\n        torch.save(D_B.state_dict(), f\"discriminator_photo_epoch{epoch+1}.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T21:32:07.061862Z","iopub.execute_input":"2025-04-06T21:32:07.062111Z","iopub.status.idle":"2025-04-06T23:51:15.272108Z","shell.execute_reply.started":"2025-04-06T21:32:07.062091Z","shell.execute_reply":"2025-04-06T23:51:15.270894Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate and save images\nfrom torchvision.utils import save_image\nfor i in range(100):\n    _, real_photo_sample = next(iter(dataloader))\n    fake_image = G_A2B(real_photo_sample.to(device)).cpu()\n    output_dir = \"/kaggle/working/\"\n    save_path = os.path.join(output_dir, f\"image_{i:04}.png\")\n    save_image(fake_image, save_path, normalize=True)\n\nwith ZipFile(zip_filename, 'w') as zipf:\n    for root, _, files in os.walk(output_dir):\n        for file in files:\n            file_path = os.path.join(root, file)\n            zipf.write(file_path, os.path.relpath(file_path, output_dir))    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T23:51:15.273250Z","iopub.execute_input":"2025-04-06T23:51:15.273582Z","iopub.status.idle":"2025-04-06T23:51:33.358770Z","shell.execute_reply.started":"2025-04-06T23:51:15.273540Z","shell.execute_reply":"2025-04-06T23:51:33.357531Z"}},"outputs":[],"execution_count":null}]}